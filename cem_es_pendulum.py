# -*- coding: utf-8 -*-
"""CEM-ES pendulum.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gtrFFkRwuK8D9APdDtT6Y0t4R3mVAOpH
"""

import gym
import numpy as np
import matplotlib.pyplot as plt

env = gym.make('Pendulum-v0')
observation_size = env.observation_space.shape[0]
action_size = env.action_space.low.shape[0]
hidden_size = observation_size * action_size * 2 #we need one parameter set for the action distribution's mean and one for its standard deviation

iterations = 1000 #amount of updates to be performed
population = 20 #amount of parameters to keep track of
keep_count = 4 #the best keep_count thetas are used for the future
max_timesteps = 500 #maximum amount of steps in an environment. In CartPole-v1, this is 500.


theta = np.array([hidden_size]) #4 inputs, two outputs (observation and action), one for each output
mean = np.random.uniform(0,1,hidden_size) #the mean for the gaussian distribution
stdev = np.random.uniform(0,1,hidden_size) #standard deviation for gaussian distribution

def preprocess(state):
    #Preprocesses the state into a horizontal vector.
    return np.reshape(state, [1,observation_size])

total_rewards = []
for i in range(iterations):
    diag = np.diag(stdev) #diagonal matrix of stdev
    theta = np.random.multivariate_normal(mean,diag,population) #samples population parameter sets given the current mean and standard deviation
    results = [] #results of each rollout
    best = [] #takes the keep_count best thetas
    for e in theta:
        s = preprocess(env.reset()) #lets our feedforward network manipulate the state
        d = False
        rewards = 0
        W = np.split(e,2)
        W_mean = np.reshape(W[0],[observation_size,action_size]) #reshapes the parameter vector to compute actions
        W_stdev = np.reshape(W[1],[observation_size,action_size]) #reshapes the parameter vector to compute actions
        for t in range(max_timesteps):
	    #env.render()
            actionMean = np.matmul(s,W_mean)
            actionStdev = np.matmul(s,W_stdev)
            a = np.random.normal(actionMean, np.abs(actionStdev), action_size)
            ns, r, d, _ = env.step(a)
            s = preprocess(ns)
            rewards += r
            if d:
                results.append([e,rewards])
                break
        if t == max_timesteps and not d:
            results.append([e,rewards])
    print('Iteration ' + str(i) +' finished with reward ' + str(results[np.argmax(np.asarray(results)[:,1])][1]))
    total_rewards.append(results[np.argmax(np.asarray(results)[:,1])][1]) #saves the reward to graph later
    for b in range(keep_count): #takes the best keep_count thetas
        best.append(results[np.argmax(np.asarray(results)[:,1])][0]) #add the best one to a new list
        results.pop(np.argmax(np.asarray(results)[:,1])) #take out the best one from the old list 
    #Creates the new mean and standard deviation based off of our best samples
    mean = np.mean(best,axis=0)
    stdev = np.std(best,axis=0)
    
plt.plot(total_rewards)
plt.show()

